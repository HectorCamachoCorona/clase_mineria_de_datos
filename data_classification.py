# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fCYZ67_8ABMFTDudJR9vdMMmjjT1bfBm

# New Section
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import mode
from typing import List
from collections import Counter


def scatter_group_by(df: pd.DataFrame, x_column: str, y_column: str, label_column: str, new_points_df: pd.DataFrame = None):
    fig, ax = plt.subplots()
    labels = pd.unique(df[label_column])
    colors = ['red', 'green', 'blue']

    for i, label in enumerate(labels):
        filtered = df[df[label_column] == label]
        ax.scatter(filtered[x_column], filtered[y_column], label=label, color=colors[i % len(colors)], alpha=0.6)

    if new_points_df is not None:
        for i, row in new_points_df.iterrows():
            ax.scatter(row[x_column], row[y_column], color='black', marker='x', s=100)
            ax.text(row[x_column]+0.2, row[y_column]+10000, row['predicted'], fontsize=9)

    ax.set_xlabel(x_column)
    ax.set_ylabel(y_column)
    ax.legend()
    plt.savefig("knn_price_year_full.png")
    plt.close()

def euclidean_distance(p1: np.array, p2: np.array) -> float:
    return np.sqrt(np.sum((p2 - p1) ** 2))

def k_nearest_neighbors(
    points: List[np.array], labels: List[str], input_data: List[np.array], k: int
):
    labels_series = pd.Series(labels)
    label_indices = {label: idx for idx, label in enumerate(labels_series.unique())}
    indices_labels = {idx: label for label, idx in label_indices.items()}

    predictions = []
    for input_point in input_data:
        distances = [euclidean_distance(input_point, point) for point in points]
        k_indices = np.argsort(distances)[:k]
        k_labels = [label_indices[labels[idx]] for idx in k_indices]
        most_common_idx = Counter(k_labels).most_common(1)[0][0]
        predictions.append(indices_labels[most_common_idx])
    return predictions


df = pd.read_csv("datos_limpios.csv")
df['year'] = pd.to_datetime(df['year'], errors='coerce')
df = df.dropna(subset=['year'])
df['year'] = df['year'].dt.year
df = df[df['selling_price'] < 5000000]

bins = [0, 300000, 700000, df['selling_price'].max()]
labels_price = ['barato', 'medio', 'caro']
df['price_label'] = pd.cut(df['selling_price'], bins=bins, labels=labels_price, include_lowest=True)


df_knn = df[['year', 'selling_price', 'price_label']].dropna()
points = [np.array([row['year'], row['selling_price']]) for _, row in df_knn.iterrows()]
labels = df_knn['price_label'].tolist()

new_points = [
    np.array([2010, 150000]),
    np.array([2015, 400000]),
    np.array([2022, 1000000]),
    np.array([2005, 100000]),
    np.array([2018, 600000])
]
predictions = k_nearest_neighbors(points, labels, new_points, k=5)

new_df = pd.DataFrame(new_points, columns=['year', 'selling_price'])
new_df['predicted'] = predictions


scatter_group_by(df_knn, "year", "selling_price", "price_label", new_points_df=new_df)

for pt, pred in zip(new_points, predictions):
    print(f"Punto {pt} fue clasificado como: {pred}")